{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# CRF based segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "关于CRF及CRF工具使用的示例见本项目/crf，这里不再赘述。本例演示使用CRF工具用于中文分词任务中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. 语料\n",
    "不管使用HMM、CRF还是其他的模型，对于中文分词任务，训练数据是少不了的。网上有公开的北大数据集（1998年1月份的《人民日报》语料），在网上可以下载到，本例也使用该语料（本例目录/data/pfr_199801.txt）。先看一眼这个语料："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迈向/v  充满/v  希望/n  的/u  新/a  世纪/n  ——/w  一九九八年/t  新年/t  讲话/n  （/w  附/v  图片/n  １/m  张/q  ）/w  \r\n",
      "中共中央/nt  总书记/n  、/w  国家/n  主席/n  江/nr  泽民/nr  \r\n",
      "（/w  一九九七年/t  十二月/t  三十一日/t  ）/w  \r\n",
      "１２月/t  ３１日/t  ，/w  中共中央/nt  总书记/n  、/w  国家/n  主席/n  江/nr  泽民/nr  发表/v  １９９８年/t  新年/t  讲话/n  《/w  迈向/v  充满/v  希望/n  的/u  新/a  世纪/n  》/w  。/w  （/w  新华社/nt  记者/n  兰/nr  红光/nr  摄/Vg  ）/w  \r\n",
      "同胞/n  们/k  、/w  朋友/n  们/k  、/w  女士/n  们/k  、/w  先生/n  们/k  ：/w  \r\n"
     ]
    }
   ],
   "source": [
    "!head -5 ./data/pku_199801.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. 语料转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. 随机切分训练集和测试集\n",
    "将北大数据集随机切分为训练集和测试集，以确保评测的可靠性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97584 ./data/pku_199801.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l ./data/pku_199801.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 打乱顺序.\n",
    "!cat ./data/pku_199801.txt | shuf > ./data/pku_199801.txt.shuf\n",
    "\n",
    "# 前7万行作为训练集.\n",
    "!head -70000 ./data/pku_199801.txt.shuf > ./data/pku_train.txt\n",
    "\n",
    "# 剩余的行作为测试集.\n",
    "!tail -27584 ./data/pku_199801.txt.shuf > ./data/pku_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. 转换为序列标注的数据格式 \n",
    "基于CRF方法的中文分词任务是一个序列标注，对每个单字标注一个label，然后顺序扫描label得到分词结果，常用的是4-tag（即BEMS）标注：  \n",
    "* 词首，常用B表示  \n",
    "* 词中，常用M表示  \n",
    "* 词尾，常用E表示  \n",
    "* 单子词，常用S表示  \n",
    "\n",
    "因此对于以上的标注语料，需要转换成BEMS的序列标注形式，可以使用以下脚本完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import sys\n",
    "\n",
    "def tagging_format(input_file, output_file):\n",
    "    input_fh = codecs.open(input_file)\n",
    "    output_fh = codecs.open(output_file, 'w', encoding='utf-8')\n",
    "\n",
    "    for line in input_fh.readlines():\n",
    "        node_list = line.strip().decode('utf8').split(' ')\n",
    "        for node in node_list:\n",
    "            word_pos = node.split('/')\n",
    "            if len(word_pos) < 2:\n",
    "                continue\n",
    "\n",
    "            word, POS = word_pos[0:2]\n",
    "            if len(word) == 1:\n",
    "                output_fh.write('%s\\t%s\\t%s\\n' % (word, POS, 'S'))\n",
    "            else:\n",
    "                output_fh.write('%s\\t%s\\t%s\\n' % (word[0], POS, 'B'))\n",
    "                for w in word[1:len(word)-1]:\n",
    "                    output_fh.write('%s\\t%s\\t%s\\n' % (w, POS, 'M'))\n",
    "                output_fh.write('%s\\t%s\\t%s\\n' % (word[len(word)-1], POS, 'E'))\n",
    "        output_fh.write('\\n')\n",
    "\n",
    "    input_fh.close()\n",
    "    output_fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagging_format('./data/pku_train.txt', './data/pku_train_4tag.txt')\n",
    "tagging_format('./data/pku_test.txt', './data/pku_test_4tag.txt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "得到4-tag格式的数据，展示一部分如下，第二列为当前字所在的词的词性，该列也有使用其他标记的（例如：汉字CN，数字NUM，标点符号PUNC，英文字符L等），该列作为模型特征的一部分。句子用空行分隔。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中\tnt\tB\r\n",
      "国\tnt\tM\r\n",
      "队\tnt\tE\r\n",
      "优\tn\tB\r\n",
      "势\tn\tE\r\n",
      "明\ta\tB\r\n",
      "显\ta\tE\r\n",
      "，\tw\tS\r\n",
      "越\tl\tB\r\n",
      "战\tl\tM\r\n",
      "越\tl\tM\r\n",
      "勇\tl\tE\r\n",
      "，\tw\tS\r\n",
      "９\tm\tS\r\n",
      "号\tq\tS\r\n",
      "孙\tnr\tS\r\n",
      "雯\tnr\tS\r\n",
      "与\tc\tS\r\n",
      "１\tm\tB\r\n",
      "３\tm\tE\r\n"
     ]
    }
   ],
   "source": [
    "!head -20 ./data/pku_train_4tag.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 特征模板\n",
    "采用常见的针对3列数据的模板文件，不在赘述具体含义，可以参考其他资料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Unigram\r\n",
      "U00:%x[-2,0]\r\n",
      "U01:%x[-1,0]\r\n",
      "U02:%x[0,0]\r\n",
      "U03:%x[1,0]\r\n",
      "U04:%x[2,0]\r\n",
      "U05:%x[-1,0]/%x[0,0]\r\n",
      "U06:%x[0,0]/%x[1,0]\r\n",
      "\r\n",
      "U10:%x[-2,1]\r\n",
      "U11:%x[-1,1]\r\n",
      "U12:%x[0,1]\r\n",
      "U13:%x[1,1]\r\n",
      "U14:%x[2,1]\r\n",
      "U15:%x[-2,1]/%x[-1,1]\r\n",
      "U16:%x[-1,1]/%x[0,1]\r\n",
      "U17:%x[0,1]/%x[1,1]\r\n",
      "U18:%x[1,1]/%x[2,1]\r\n",
      "\r\n",
      "U20:%x[-2,1]/%x[-1,1]/%x[0,1]\r\n",
      "U21:%x[-1,1]/%x[0,1]/%x[1,1]\r\n",
      "U22:%x[0,1]/%x[1,1]/%x[2,1]\r\n",
      "\r\n",
      "# Bigram\r\n",
      "B\r\n"
     ]
    }
   ],
   "source": [
    "!cat template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 评估脚本\n",
    "crfsgd工具训练时必须指定一个评估的钩子程序，默认使用的是conlleval，但是实验发现conlleval对此任务中定义的BEMS标签无效，可能需要改一下脚本，因为perl都忘了，所以没有花时间去修改，而是用python写了一个简单的评估脚本（`./seg_eval.py`），如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\r\n",
      "# -*- coding: utf-8 -*-\r\n",
      "import sys\r\n",
      " \r\n",
      "if __name__==\"__main__\":\r\n",
      "    pred_word_count = 0\r\n",
      "    target_word_count = 0\r\n",
      "    correct_word_count = 0\r\n",
      "    for line in sys.stdin:\r\n",
      "        fields = line.strip().decode('utf8').split()\r\n",
      "        if len(fields) != 4:\r\n",
      "            continue\r\n",
      "    \r\n",
      "        target, pred = fields[2:4]\r\n",
      "        if pred in ('E', 'S'):\r\n",
      "            pred_word_count += 1\r\n",
      "            if target == pred:\r\n",
      "                correct_word_count +=1\r\n",
      "    \r\n",
      "        if target in ('E', 'S'):\r\n",
      "            target_word_count += 1\r\n",
      " \r\n",
      "    P = correct_word_count / float(pred_word_count)\r\n",
      "    R = correct_word_count / float(target_word_count)\r\n",
      "    F1 = (2 * P * R) / (P + R)\r\n",
      "    \r\n",
      "    print('  --> Word count of predict, golden and correct : %d, %d, %d' %\r\n",
      "            (pred_word_count, target_word_count, correct_word_count))\r\n",
      "    print(\"  --> P = %f, R = %f, F1 = %f\" % (P, R, F1))\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./seg_eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5. 训练与评估\n",
    "使用crfsgd工具进行训练，说明：  \n",
    "* 参数-h 设置进行评估的步数（默认5步），会对训练集进行评估，如果同时传入测试文件，也会对训练集进行评估；\n",
    "* 参数-e 设置评估命令，默认为conlleval；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading template file template.\n",
      "  u-templates: 19  b-templates: 1\n",
      "Scanning data/pku_train_4tag.txt to build dictionary.\n",
      "  sentences: 69887  outputs: 4\n",
      "  cutoff: 3  features: 576663  parameters: 2306664\n",
      "  duration: 60.65 seconds.\n",
      "Using c=1, i.e. lambda=1.43088e-05\n",
      "Reading and preprocessing data/pku_train_4tag.txt.\n",
      "  processed: 69887 sentences.\n",
      "  duration: 77.94 seconds.\n",
      "Reading and preprocessing data/pku_test_4tag.txt.\n",
      "  processed: 27533 sentences.\n",
      "  duration: 31.24 seconds.\n",
      "[Calibrating] --  1000 samples\n",
      " initial objective=130.454\n",
      " trying eta=0.1  obj=4.014 (possible)\n",
      " trying eta=0.2  obj=8.76199 (possible)\n",
      " trying eta=0.4  obj=15.7913 (possible)\n",
      " trying eta=0.8  obj=40.1677 (possible)\n",
      " trying eta=1.6  obj=92.9449 (possible)\n",
      " trying eta=3.2  obj=125.984 (possible)\n",
      " trying eta=6.4  obj=327.972 (too large)\n",
      " trying eta=0.05  obj=2.78034 (possible)\n",
      " trying eta=0.025  obj=3.70322 (possible)\n",
      " trying eta=0.0125  obj=4.9997 (possible)\n",
      " trying eta=0.00625  obj=6.72676 (possible)\n",
      " taking eta=0.025  t0=2.79548e+06 time=5.33s.\n",
      "[Epoch 1] -- wnorm=4841.44 time=28.68s.\n",
      "Training perf: sentences=69887 loss=0.97187 obj=1.00651 err=43868 (0.659032%)\n",
      "  --> Word count of predict, golden and correct : 4026062, 4034945, 4005097\n",
      "  --> P = 0.994793, R = 0.992603, F1 = 0.993696\n",
      "Testing perf: sentences=27533 loss=1.07605 obj=1.11069 err=19317 (0.743521%)\n",
      "  --> Word count of predict, golden and correct : 1568523, 1572290, 1559273\n",
      "  --> P = 0.994103, R = 0.991721, F1 = 0.992910\n",
      "[Epoch 2] -- wnorm=7545.73 time=51.96s.\n",
      "Training perf: sentences=69887 loss=0.631196 obj=0.685182 err=22744 (0.341685%)\n",
      "  --> Word count of predict, golden and correct : 4033188, 4034945, 4021708\n",
      "  --> P = 0.997154, R = 0.996719, F1 = 0.996936\n",
      "Testing perf: sentences=27533 loss=0.742075 obj=0.796061 err=11511 (0.443064%)\n",
      "  --> Word count of predict, golden and correct : 1571257, 1572290, 1565482\n",
      "  --> P = 0.996325, R = 0.995670, F1 = 0.995997\n",
      "[Epoch 3] -- wnorm=9679.58 time=75.89s.\n",
      "Training perf: sentences=69887 loss=0.45451 obj=0.523762 err=14709 (0.220974%)\n",
      "  --> Word count of predict, golden and correct : 4031152, 4034945, 4024175\n",
      "  --> P = 0.998269, R = 0.997331, F1 = 0.997800\n",
      "Testing perf: sentences=27533 loss=0.563952 obj=0.633204 err=8186 (0.315083%)\n",
      "  --> Word count of predict, golden and correct : 1570263, 1572290, 1566380\n",
      "  --> P = 0.997527, R = 0.996241, F1 = 0.996884\n",
      "[Epoch 4] -- wnorm=11439.8 time=99.69s.\n",
      "Training perf: sentences=69887 loss=0.376037 obj=0.457882 err=10842 (0.16288%)\n",
      "  --> Word count of predict, golden and correct : 4032895, 4034945, 4027550\n",
      "  --> P = 0.998675, R = 0.998167, F1 = 0.998421\n",
      "Testing perf: sentences=27533 loss=0.482463 obj=0.564308 err=6738 (0.259349%)\n",
      "  --> Word count of predict, golden and correct : 1571030, 1572290, 1567720\n",
      "  --> P = 0.997893, R = 0.997093, F1 = 0.997493\n",
      "[Epoch 5] -- wnorm=12935 time=122.54s.\n",
      "Training perf: sentences=69887 loss=0.323263 obj=0.415806 err=7684 (0.115437%)\n",
      "  --> Word count of predict, golden and correct : 4033081, 4034945, 4029453\n",
      "  --> P = 0.999100, R = 0.998639, F1 = 0.998870\n",
      "Testing perf: sentences=27533 loss=0.42243 obj=0.514972 err=5386 (0.20731%)\n",
      "  --> Word count of predict, golden and correct : 1571004, 1572290, 1568437\n",
      "  --> P = 0.998366, R = 0.997549, F1 = 0.997958\n",
      "Saving model file model/seg.model.\n",
      "Done!  122.54 seconds.\n"
     ]
    }
   ],
   "source": [
    "!./bin/crfsgd -c 1.0 -f 3 -r 5 -h 1 -e \"./seg_eval.py\" model/seg.model template data/pku_train_4tag.txt data/pku_test_4tag.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 使用训练的模型 **  \n",
    "对已经训练得到的模型，可以直接加载并使用，参数-t指定模型文件。下面使用上面训练的模型`model/deg.model`对测试集`data/pku_test_4tag.txt`进行评估，可以看到评估结果与上面训练阶段最后一步对测试集的评估结果是一样的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> Word count of predict, golden and correct : 1571004, 1572290, 1568437\r\n",
      "  --> P = 0.998366, R = 0.997549, F1 = 0.997958\r\n"
     ]
    }
   ],
   "source": [
    "!./bin/crfsgd -t model/seg.model data/pku_test_4tag.txt | ./seg_eval.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
