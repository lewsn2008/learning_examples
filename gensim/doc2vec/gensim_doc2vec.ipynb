{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim doc2vec example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 本例是一个基于gensim中的doc2vec来做实体消歧任务的例子。举个例子，“非诚勿扰”有综艺节目和电影两个不同的词义，分别标记为\"enter_非诚勿扰\"和\"movie_非诚勿扰\"，使用标记有这两种tag的句子作为训练语料，使用gensim的doc2vec训练出两种tag的embedding，测试阶段使用doc2ved的infer_vector计算出句子的embedding，计算与两种tag之间的cosine similarity，取最近的作为词义结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python                                                                                                                                                                                             \n",
    "#coding=utf8\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 这两行需要注释掉，不然会影响jupyter中print的输出.\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding(\"utf8\")\n",
    "\n",
    "DATE_PTN = re.compile(u'(((19)*9\\d|(20)*[01]\\d)\\-?)?((0[1-9]|1[012])\\-?)([012]\\d|3[01])')\n",
    "def_labels = ['movie', 'episode', 'enter', 'cartoon', 'game']\n",
    "label_to_index = {'movie':0, 'episode':1, 'enter':2, 'cartoon':3, 'game':4}\n",
    "index_to_label = {0:'movie', 1:'episode', 2:'enter', 3:'cartoon', 4:'game'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> data目录中有训练语料wsd_train_data和测试语料wsd_test_data. 每行为一条句子，所有字段以tab分隔，第一列为词义类型（本例中定义了五种类型：电影、电视剧、综艺、动漫、游戏）, 第二列为待消歧词，第三列往后为句子分词后所有的词."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie\t非诚勿扰\t《\t非诚勿扰\t2\t》\t直面\t七年之痒\t葛优\t大秀\t经典\t台词\r\n",
      "enter\t非诚勿扰\t非诚勿扰\t@date@\t美女\t调侃\t孟非\t扮\t法海\t收\t青蛇\r\n",
      "enter\t非诚勿扰\t江苏卫视\t《\t非诚勿扰\t》\t刘佳妮\t牵手\t精彩\t视频\r\n",
      "movie\t非诚勿扰\t花絮\t《\t非诚勿扰\t》\t导演\t冯小刚\t工作\t状态\t曝光\r\n"
     ]
    }
   ],
   "source": [
    "!head -4 data/wsd_train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 定义读取语料的类，该类从文件中按行读取语料，转换为doc2vec所需的LabeledSentence，保存加载后的语料到类变量中，self.labelled_sentences为所有的句子，self.labels为对应的词义类别."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self):\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.labels = []\n",
    "        self.labelled_sentences = []\n",
    "\n",
    "    def load(self, train_file):                                                                                                                                                                                   \n",
    "        self._reset()\n",
    "        with open(train_file) as fin:\n",
    "            for line_num, line in enumerate(fin):\n",
    "                parts = line.strip().decode('utf8').split('\\t')\n",
    "                if len(parts) < 3:\n",
    "                    continue\n",
    "\n",
    "                label, keyword = parts[0:2]\n",
    "                # 将句子中的日期字符串(如“20120813”)归一化为\"@date@\".\n",
    "                sent = ['@date@' if DATE_PTN.match(term) else term for term in parts[2:]]\n",
    "\n",
    "                # 给句子标记tag, 每个句子可以是多个tag, 可以是句子的序号(number), \n",
    "                # 或者唯一的标记(如：sent_i), 此处用的是WSD(词义消歧)中的词义为tag,\n",
    "                # 例如“花千骨”有game和episode两种词义, 对于标记了词义的句子可以打上\n",
    "                # 不同词义的tag, 例如：game_花千骨, episode_花千骨.\n",
    "                #sent_tag = 'SENT_%d' % line_num\n",
    "                keyword_tag = '%s_%s' % (label, keyword)\n",
    "\n",
    "                # 构造gensim的LabeledSentence表示一个标记了tag的句子.\n",
    "                labeled_sent = LabeledSentence(sent, [keyword_tag])\n",
    "\n",
    "                # 保存句子和分类标签.\n",
    "                self.labelled_sentences.append(labeled_sent)\n",
    "                self.labels.append(label_to_index[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 定义模型类，实现加载数据、训练、保存和加载模型、评估测试集等."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Doc2VecModel(object):\n",
    "    def __init__(self):\n",
    "        self.labelled_corpus = None\n",
    "        self.sense_tag_vec_dict = {}\n",
    "        self.doc2vec_dim = 0\n",
    "\n",
    "    def train(self, train_file, dimension, epoch):\n",
    "        if not train_file:\n",
    "            print('Train file path needed!')\n",
    "            sys.exit(1)\n",
    "\n",
    "        # Load corpus from file.\n",
    "        self.labelled_corpus = LabeledLineSentence()\n",
    "        self.labelled_corpus.load(train_file)\n",
    "\n",
    "        # Define doc2vec object and build vocabulary.\n",
    "        labelled_sents = self.labelled_corpus.labelled_sentences\n",
    "        self.doc2vec = Doc2Vec(min_count=1, size=dimension, window=15)\n",
    "        self.doc2vec.build_vocab(labelled_sents)\n",
    "\n",
    "        # Train doc2vec.\n",
    "        for epoch in range(epoch):\n",
    "            print('epoch %d ...' % epoch)\n",
    "            shuffle(labelled_sents)\n",
    "            self.doc2vec.train(labelled_sents)\n",
    "\n",
    "        self.doc2vec_dim = dimension\n",
    "        self.__save_sense_tag_vecs()\n",
    "        print('Finished training!')\n",
    "                                                                                                                                                                                                                  \n",
    "    def dump(self, model_file):\n",
    "        if model_file:\n",
    "            self.doc2vec.save(model_file)\n",
    "\n",
    "    def load(self, model_file):\n",
    "        if model_file:\n",
    "            self.doc2vec = Doc2Vec.load(model_file)\n",
    "        self.doc2vec_dim = self.doc2vec.docvecs[0].shape[0]\n",
    "        self.__save_sense_tag_vecs()\n",
    "    \n",
    "    def __save_sense_tag_vecs(self):\n",
    "        for i in range(len(self.doc2vec.docvecs)):\n",
    "            sense_tag = self.doc2vec.docvecs.index_to_doctag(i)\n",
    "            sense_tag_vec = self.doc2vec.docvecs[i]\n",
    "            self.sense_tag_vec_dict[sense_tag] = (\n",
    "                sense_tag_vec / np.linalg.norm(sense_tag_vec))\n",
    "\n",
    "    def eval_test(self, test_file):\n",
    "        with open(test_file) as fin:\n",
    "            correct_num = 0\n",
    "            for line_num, line in enumerate(fin):\n",
    "                parts = line.strip().decode('utf8').split('\\t')\n",
    "                if len(parts) < 3:\n",
    "                    continue\n",
    "\n",
    "                target_label, keyword = parts[0:2]\n",
    "                sent = ['@date@' if DATE_PTN.match(term) else term for term in parts[2:]]\n",
    "                if keyword not in sent:\n",
    "                    continue\n",
    "\n",
    "                # Context text embedding similarities {{{.\n",
    "                sent_vec = self.doc2vec.infer_vector(\n",
    "                        sent, alpha=0.1, min_alpha=0.0001, steps=5)\n",
    "                sent_vec = sent_vec / np.linalg.norm(sent_vec)                                                                                                                                                    \n",
    "                sense_tag_vecs = self.get_sense_tag_vecs(parts[1])\n",
    "                sims = sent_vec.dot(sense_tag_vecs.T)\n",
    "                sims = (sims + 1) / 2   # normalization\n",
    "                # }}}.\n",
    "\n",
    "                pred_label = np.argmax(sims)\n",
    "                if label_to_index[target_label] == pred_label:\n",
    "                    correct_num += 1\n",
    "\n",
    "        print('Eval on test set: precision : %f (%d/%d)' %\n",
    "            (correct_num/float(line_num), correct_num, line_num))\n",
    "        \n",
    "    def get_sense_tag_vecs(self, word):\n",
    "        if not isinstance(word, unicode):\n",
    "            word = word.decode('utf8')\n",
    "\n",
    "        default_vec = np.zeros(self.doc2vec_dim)\n",
    "        vecs = []\n",
    "        for label in def_labels:\n",
    "            sense_tag = '%s_%s' % (label, word)\n",
    "            sense_tag_vec = self.sense_tag_vec_dict.get(sense_tag, default_vec)                                                                                                                                   \n",
    "            vecs.append(sense_tag_vec.tolist())\n",
    "\n",
    "        return np.array(vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练与保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 ...\n",
      "epoch 1 ...\n",
      "epoch 2 ...\n",
      "epoch 3 ...\n",
      "epoch 4 ...\n",
      "Finished training!\n"
     ]
    }
   ],
   "source": [
    "model = Doc2VecModel()\n",
    "model.train('./data/wsd_train_data', 200, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.dump('./model/wsd_train_data_200.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Doc2VecModel()\n",
    "model.load('./model/wsd_train_data_200.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval on test set: precision : 0.803762 (1581/1967)\n"
     ]
    }
   ],
   "source": [
    "model.eval_test('./data/wsd_test_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
